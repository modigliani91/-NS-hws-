{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a255986",
   "metadata": {},
   "source": [
    "# Assignment â€” (Sub)graph-level tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848c6403",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from dgl.data import DGLDataset\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch import nn\n",
    "from dgl.nn import GraphConv\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "import pandas as pd\n",
    "import dgl\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Subset\n",
    "from dgl.nn import GraphConv, AvgPooling\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from IPython.display import clear_output\n",
    "from sklearn.manifold import TSNE\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b617d9d7",
   "metadata": {},
   "source": [
    "### Task 1. Dataset description (0 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc565f46-1411-4fd2-8bcb-b2b1d46dac84",
   "metadata": {},
   "source": [
    "Previously we have studied the graph neural networks that allow us to obtain node embeddings. However, there are several (sub)graph-level tasks, e.g. molecular property prediction. To solve it, we should be able to receive full graph embedding. This seminar concentrates on several graph pooling techniques.\n",
    "\n",
    "We will experiment with the [Graph Property Prediction dataset](https://ogb.stanford.edu/docs/graphprop/), more precisely with `ogbg-molhiv`.\n",
    "The dataset contains several graphs adopted from [MoleculeNet](https://pubs.rsc.org/en/content/articlehtml/2018/sc/c7sc02664a), each graph represents the molecule, where nodes are atoms, and edges are chemical bonds. Input node features are 9-dimensional, containing atomic number and chirality, as well as other additional atom features such as formal charge and whether the atom is in the ring or not.\n",
    "\n",
    "In the next cell we load dataset from `ogb` library, split it into train, validation and test subsets and create pytorch dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55f80cc-4066-47fc-836f-55c8290bb6c9",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install ogb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17ffb5a-9070-495a-be9d-c82340980a2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ogb.graphproppred.mol_encoder import AtomEncoder, BondEncoder\n",
    "from ogb.graphproppred import DglGraphPropPredDataset, collate_dgl, Evaluator\n",
    "\n",
    "dataset = DglGraphPropPredDataset(name=\"ogbg-molhiv\")\n",
    "\n",
    "split_idx = dataset.get_idx_split()\n",
    "train_loader = DataLoader(dataset[split_idx[\"train\"]], batch_size=32, shuffle=True, collate_fn=collate_dgl)\n",
    "valid_loader = DataLoader(dataset[split_idx[\"valid\"]], batch_size=32, shuffle=False, collate_fn=collate_dgl)\n",
    "test_loader = DataLoader(dataset[split_idx[\"test\"]], batch_size=32, shuffle=False, collate_fn=collate_dgl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9400c990-b98e-4584-b54e-38b0948b91c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "labels = dataset.labels.detach().cpu().numpy()\n",
    "pos = np.where(labels)[0]  # choose positive labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d1dfed-c6de-4f39-ae3a-0d924e78f634",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nx.draw_kamada_kawai(dataset.graphs[0].to_networkx())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dcc535-5d92-46d0-9e9a-9ed7509b47e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nx.draw_kamada_kawai(dataset.graphs[1].to_networkx())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ee7177-c1e7-48db-8646-f84801d1c8aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nx.draw_kamada_kawai(dataset.graphs[pos[0]].to_networkx())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39658735-272c-472c-a686-9b055bb664f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nx.draw_kamada_kawai(dataset.graphs[pos[1]].to_networkx())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24def3a",
   "metadata": {},
   "source": [
    "### Task 2. DGL for multiple graphs (0 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a573aa-2101-4050-b796-33f67872f9df",
   "metadata": {},
   "source": [
    "Previously, we have used DGL to work with one graph. However, it is also suitable for working in multiple graphs scenario.\n",
    "\n",
    "DataLoader encodes multiple graphs as one large graph. Nodes of it each graph are grouped. To reconstruct whether the specific graph is, we can use `graph.batch_num_nodes()`. This function will return tensor with number of nodes in each graph.\n",
    "\n",
    "For example, in the next cell we will extract the first element from the `train_loader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3df112-268f-4dd3-95dc-2518a868b38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs, labels = train_loader.__iter__().__next__()\n",
    "graphs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316703ca-088f-4370-b36b-950731614e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs.batch_num_nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d828f3de-44f9-4f2d-9331-618db8e3d5f9",
   "metadata": {},
   "source": [
    "From that we can reconstruct the graph indicator and manually aggregate data over it. However DGL has several predefined functions for this case: `dgl.sum_nodes`, `dgl.mean_nodes`, `dgl.max_nodes`, `dgl.min_nodes`. These functions have similar signatures, e.g., `dgl.sum_nodes(graph, \"feat\")`. The second parameter aggregates the attribute name in `graph.ndata` which is a dictionary that contains node-level properties as a `torch.tensor` for all vertices. You can assign any value here. It is usefull when we use message-passing approach to pass the hidden features.\n",
    "\n",
    "For example, let us check what data it currently contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61979e18-372b-4741-b536-97996c97a13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs.ndata['feat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca262f0-cb46-40f7-aab9-66fc9ab6b088",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs.ndata['feat'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e44c3b5-cfc6-49d4-9b52-cb777929b64d",
   "metadata": {},
   "source": [
    "Currently, graph has only default features under the `\"feat\"` parameter. The pooling functions works only with float features but `\"feat\"` are categorical (Long) labels. For demostration purpose we can convert it to `torch.FloatTensor` and assign to the new field named `\"float_feat\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab59a9e-400d-4229-b779-c631843e32ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs.ndata[\"float_feat\"] = graphs.ndata[\"feat\"].to(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc93eae4-0c99-45ec-9df4-4f6b43b3f42b",
   "metadata": {},
   "source": [
    "Now, we are able to aggregate graph features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4f8f0c-58c7-4650-96e0-e5d66cb763dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = dgl.mean_nodes(graphs, \"float_feat\")\n",
    "mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85aa272-3ea5-4227-98f0-66b99d2708a3",
   "metadata": {},
   "source": [
    "Let us check that number of lines is equal to the number of graphs in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc03a341-a15a-4e42-8c29-356e441ce0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert mean.size(0) == train_loader.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717b7c5d",
   "metadata": {},
   "source": [
    "### Task 3. Sum, mean and max pooling (0 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2706e2-fb19-416d-8723-ec6ee77d5352",
   "metadata": {},
   "source": [
    "Previously, we have studied GNNs. To receive node embeddings, usually, one use some operator that aggregates adjacent node embeddings. We can use similar approach for the whole graphs. In the task, we will implement the classic methods of graph-level pooling with further prediction of graph class, e.g, whether a molecule inhibits HIV virus replication or no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108d5906-862d-4173-96fb-2417a4811cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePooling(nn.Module):\n",
    "    def __init__(self, atom_dim=8, out_dim=1, pooling_func='sum'):\n",
    "        \"\"\"\n",
    "        Simple neural network that encodes atoms, pool it with sum, max or mean aggregation.\n",
    "        In the task you need to instantiate AtomEncoder, pooling function\n",
    "        and projection (fully-connected) layer from embedding dimension to output dimension\n",
    "\n",
    "        :param atom_dim: dimension of atom embedding for AtomEncoder (int)\n",
    "        :param out_dim: dimension of output (int)\n",
    "        :param pooling_func: name of pooling function (str)\n",
    "                             could be one of 'sum', 'mean' or 'max'.\n",
    "                             You need to use proper aggregation function described above\n",
    "        \"\"\"\n",
    "        super(SimplePooling, self).__init__()\n",
    "        self.atom_encoder = AtomEncoder(emb_dim=atom_dim)\n",
    "        self.pooling_func = getattr(dgl, f\"{pooling_func}_nodes\")\n",
    "        self.fc = nn.Linear(atom_dim, out_dim)\n",
    "\n",
    "    def forward(self, graph):\n",
    "        \"\"\"\n",
    "        Encodes the atom in the graph, pool vertices over graphs and predict the target\n",
    "\n",
    "        :param graph: dgl graph with batch of graphs\n",
    "        \"\"\"\n",
    "        graph.ndata['h'] = self.atom_encoder(graph.ndata[\"feat\"])\n",
    "        return self.fc(self.pooling_func(graph, 'h'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f291d6c9-8466-4dab-9464-2aea78977f87",
   "metadata": {},
   "source": [
    "We are trying to solve classification task, so we can use the binary cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993a1b56-eb54-4f89-a319-b41c480cd570",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb1b4c0-de51-43d7-ae1a-cc8af8150fa3",
   "metadata": {},
   "source": [
    "Let us define the `train` method. It iterates over batches and for each batch do the following:\n",
    "1. calculates forward pass with model\n",
    "2. moves to zero the gradient of previous step in optimizer\n",
    "3. calculates the loss according to the criterion\n",
    "4. backwards the loss\n",
    "5. does optimization step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea9b3b2-c019-47b6-a23f-d6369980433a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "def train(model, loader, optimizer):\n",
    "    \"\"\"\n",
    "    Train loop for model. Do not forget to switch model to train mode and place all tensors to device\n",
    "\n",
    "    :param model: neural net that we want to train\n",
    "    :param loader: data loader\n",
    "    :param optimizer: optimizer for given network\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    for batch, label in tqdm(loader):\n",
    "        batch = batch.to(device)\n",
    "        batch.ndata['feat'] = batch.ndata['feat'].to(device)\n",
    "        batch.edata['feat'] = batch.edata['feat'].to(device)\n",
    "        label = label.to(device)\n",
    "        logits = model(batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(logits.to(torch.float32), label.to(torch.float32))\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4174353-3c1b-4fd6-9c81-c08511875ab2",
   "metadata": {},
   "source": [
    "We also need the evaluation method, which will return the metrics for the task. The `ogb` library already has the `Evaluator` class. Let us instantiate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c9d8f2-516c-4a8b-9ef8-f7e29cbefdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(\"ogbg-molhiv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5572d8-d505-4f67-9127-2930d949ec32",
   "metadata": {},
   "source": [
    "Evaluator.eval method takes the python dict with ground truth (`y_true` key) and predicted (`y_pred` key) labels. It returns the dictionary with metrics. The `rocauc` metric for the given dataset is used. In the evaluate function we need to iterate over batches in the `loader` and predict the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227bf5eb-6936-4870-a3aa-bbdcfd7ff333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, evaluator):\n",
    "    \"\"\"\n",
    "    Evaluation method (do not forget to switch model mode to eval).\n",
    "    Calculates the `rocauc` metric.\n",
    "\n",
    "    :param model: model for validation\n",
    "    :param laoder: data loader with data for evaluation\n",
    "    :param evaluator: ogb.Evaluator for metric calculation\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for batch, label in tqdm(loader):\n",
    "        batch = batch.to(device)\n",
    "        batch.ndata['feat'] = batch.ndata['feat'].to(device)\n",
    "        batch.edata['feat'] = batch.edata['feat'].to(device)\n",
    "        label = label.to(device)\n",
    "        p = model(batch)\n",
    "        y_true.append(label.view(p.shape).detach().cpu())\n",
    "        y_pred.append(p.detach().cpu())\n",
    "    y_true = torch.cat(y_true, dim=0).numpy()\n",
    "    y_pred = torch.cat(y_pred, dim=0).numpy()\n",
    "\n",
    "    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n",
    "\n",
    "    return evaluator.eval(input_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c81b2ce-c91c-4224-856e-59e8502c2161",
   "metadata": {},
   "source": [
    "And finally, we need to define the runner function that will train and evaluate model for several epochs with early stopping, where validation score tends to descrease. Also we can use following early stopping configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4047bc21-3fa5-4015-894a-ee1c1ddd537b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, exp_name='pooling'):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.exp_name = exp_name + '_es_checkpoint.pt'\n",
    "\n",
    "    def step(self, acc, model):\n",
    "        score = acc\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(model)\n",
    "        elif score < self.best_score:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(model)\n",
    "            self.counter = 0\n",
    "        return self.early_stop\n",
    "\n",
    "    def save_checkpoint(self, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        torch.save(model.state_dict(), self.exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aafd6d8-063f-4694-a707-1bc7cb4431de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model, exp_name, patience=5, num_epochs=1):\n",
    "    \"\"\"\n",
    "    Runner function to train and evaluate the model\n",
    "\n",
    "    :param model: neural network for training and validation\n",
    "    :param exp_name: name of experiment\n",
    "    :param patience: number of rounds for early-stopping\n",
    "    :param num_epochs: number of epochs\n",
    "    \"\"\"\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "    stopper = EarlyStopping(patience, exp_name)\n",
    "    scores = {\n",
    "      \"train\": [],\n",
    "      \"validation\": [],\n",
    "      \"test\": [],\n",
    "    }\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"=======Epoch {epoch + 1}\")\n",
    "        train(model, train_loader, optimizer)\n",
    "\n",
    "        scores['train'].append(evaluate(model, train_loader, evaluator)[\"rocauc\"])\n",
    "        scores['validation'].append(evaluate(model, valid_loader, evaluator)[\"rocauc\"])\n",
    "        scores['test'].append(evaluate(model, test_loader, evaluator)[\"rocauc\"])\n",
    "\n",
    "        clear_output()\n",
    "        plt.title(f\"Score dynamics. train: {scores['train'][-1]:.4f}, validation: {scores['validation'][-1]:.4f}, test: {scores['test'][-1]:.4f}\")\n",
    "        plt.plot(scores['train'], label=\"train\")\n",
    "        plt.plot(scores['validation'], label=\"validation\")\n",
    "        plt.plot(scores['test'], label=\"test\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        if stopper.step(scores['validation'][-1], model):\n",
    "            print(f\"{exp_name} test quality: {scores['test'][-patience - 1]}\")\n",
    "            break\n",
    "        else:\n",
    "            print(f\"{exp_name} test quality: {scores['test'][-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd52325-426d-4a07-a930-f9a8b118b7b4",
   "metadata": {},
   "source": [
    "Now we can validate our pooling approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7720d9-4401-429d-9519-f162b21a9daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_model = SimplePooling(out_dim=dataset.num_tasks, pooling_func='max').to(device)\n",
    "run(max_model, \"max_pooling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e57148-b036-4a0c-8646-c93ad4a2df38",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_model = SimplePooling(out_dim=dataset.num_tasks, pooling_func='mean').to(device)\n",
    "run(mean_model, \"mean_pooling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d54ebd0-871f-407d-8be3-c26b81a38744",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_model = SimplePooling(out_dim=dataset.num_tasks, pooling_func='sum').to(device)\n",
    "run(sum_model, \"sum_pooling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14bb54f-d21e-4050-b086-a02f0ada1f25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4aa6c9d",
   "metadata": {},
   "source": [
    "### Task 4. DeepSet pooling (0 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6408d0a-38f4-4c75-8ea0-2fd2b5c27930",
   "metadata": {},
   "source": [
    "The paper: https://arxiv.org/abs/1703.06114\n",
    "\n",
    "Simple aggregation of set element embedding could produce the wrong whole graph embedding, because we try to aggregate continuously discrete structures. So, one of the methods to eliminate it is the Deep Sets.\n",
    "\n",
    "Deep Sets employs an idea that feed-forward neural networks are the universal estimators. Authors proves the theorem that for working with permutation invariant aggregation (e.g. summation) we can use as following:\n",
    "\n",
    "$$\\text{DeepSetPooling} = \\text{MLP}_\\phi \\left(\\sum_{v\\in N} \\text{MLP}_\\theta (h_v)\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7abbf0b-e97f-4888-bf81-c61609cd19b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSetPooling(nn.Module):\n",
    "    def __init__(self, atom_dim=8, num_layers_deep_set=3, hidden_dim=8, num_out_layers=2, out_dim=1):\n",
    "        super(DeepSetPooling, self).__init__()\n",
    "        self.atom_encoder = AtomEncoder(emb_dim=atom_dim)\n",
    "        self.pooling_func = dgl.sum_nodes\n",
    "        self.deepsets = [nn.Linear(atom_dim, hidden_dim)]\n",
    "        for _ in range(num_layers_deep_set - 1):\n",
    "            self.deepsets.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "        self.deepsets = nn.ModuleList(self.deepsets)\n",
    "        self.out_layers = []\n",
    "        for _ in range(num_out_layers - 1):\n",
    "            self.out_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "        self.out_layers.append(nn.Linear(hidden_dim, out_dim))\n",
    "        self.out_layers = nn.ModuleList(self.out_layers)\n",
    "    \n",
    "    def forward(self, graph):\n",
    "        graph.ndata['h'] = self.atom_encoder(graph.ndata[\"feat\"])\n",
    "        for fc in self.deepsets:\n",
    "            graph.ndata['h'] = fc(graph.ndata['h'])\n",
    "            graph.ndata['h'] = F.relu(graph.ndata['h'])\n",
    "        graph_vec = self.pooling_func(graph, 'h')\n",
    "        for fc in self.out_layers[:-1]:\n",
    "            graph_vec = fc(graph_vec)\n",
    "            graph_vec = F.relu(graph_vec)\n",
    "        return self.out_layers[-1](graph_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56c5fec-6711-49fa-b1ba-1670456d746c",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepset_model = DeepSetPooling(out_dim=dataset.num_tasks).to(device)\n",
    "run(deepset_model, \"deepset_pooling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd05ff00-78e5-481b-ae75-41c4cf9353f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c18a77b",
   "metadata": {},
   "source": [
    "### Task 5. DiffPool for graph coarsening (0 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04020a2b-a674-4fcb-968f-d512158ea421",
   "metadata": {},
   "source": [
    "The previous methods omit the graph structure because it simply pools the node embeddings.\n",
    "\n",
    "To solve this issue, we will try technique named graph coarsening, specifically we will realised simplified version of the paper [Hierarchical Graph Representation Learning with Differentiable Pooling](https://arxiv.org/abs/1806.08804). It is an iterative procedure consisting of several steps:\n",
    "\n",
    "1. Train GNN to predict node clusters (learn cluster assignment matrix $S$).\n",
    "\n",
    "  In the step we simply apply GCN layer to our graph with feature projection to a lower dimension size. Then we normalize matrix with softmax over the feature dimension. Received matrix is the $S$.\n",
    "\n",
    "2. Coarse the graph to the graph of clusters\n",
    "\n",
    "  Calculate coarse adjacency matrix projecting original matrix with assignment one\n",
    "$$A^\\text{new} = S^\\top A S$$\n",
    "  Calculate the node level features with node embedding pooling\n",
    "$$X^\\text{new} = S^\\top X$$\n",
    "3. Repeat steps 1-2 several times\n",
    "4. Use set polling technique to receive the graph embedding\n",
    "\n",
    "Due to on-flight calculation of matrix $A^\\text{new}$, it will be easier to work with GCN in vanilla way (project features with linear layer and then aggregate over the adjacency matrix).\n",
    "\n",
    "Firstly, we will define an `DiffPoolLayer`\n",
    "\n",
    "This layer realise the first two steps of the algorithm described above. More precisely, we can follow this steps to realise the `forward` method:\n",
    "\n",
    "1. It takes adjacency matrix, node features and list with number of nodes in each batch.\n",
    "2. Then it calculates the assign tensor with vanilla gcn: linear projection of hidden features, aggregation of features over the neighbours (matrix multiplication between adjacency and received feature projection)\n",
    "3. Assign tensor is normalized with softmax over the feature dimension\n",
    "4. Assign tensor is splitted into list of blocks based on number of nodes for each graph in batch\n",
    "5. Splitted assign tensor is converted to the block diagonal matrix\n",
    "6. Then it passes features through the GCN to receive new hidden representations of it.\n",
    "7. New hidden features is calculated by matrix multiplication between assign matrix and features from 6\n",
    "8. New adjacency matrix is calculated with projection by assign matrix\n",
    "9. Return new adjacency and new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e57c47e-31a3-4d90-a95c-60a063e34384",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffPoolLayer(nn.Module):\n",
    "    def __init__(self, input_dim, assign_dim, out_feature_dim):\n",
    "        super(DiffPoolLayer, self).__init__()\n",
    "        self.feature_layer = nn.Sequential(\n",
    "            nn.Linear(input_dim, out_feature_dim),\n",
    "            nn.Linear(out_feature_dim, out_feature_dim)\n",
    "        )\n",
    "        self.pooling_layer = nn.Linear(input_dim, assign_dim)\n",
    "    \n",
    "    def gcn_with_adj(self, adj, h, layer):\n",
    "        h = layer(h)\n",
    "        return adj @ h / (adj.sum(dim=1, keepdims=True) + 1e-20)\n",
    "  \n",
    "    def forward(self, adj, h, batch_num_nodes):\n",
    "        s = self.gcn_with_adj(adj, h, self.pooling_layer)\n",
    "        s = F.softmax(s, dim=1)\n",
    "        s = torch.split(s, batch_num_nodes)\n",
    "        s = torch.block_diag(*s)\n",
    "        f = self.gcn_with_adj(adj, h, self.feature_layer)\n",
    "        f_new = s.t() @ f\n",
    "        adj_new = s.t() @ adj @ s\n",
    "        return adj_new, f_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87394ee4-35ed-4e7d-9e78-82b63a4e657b",
   "metadata": {},
   "source": [
    "This layer will be used in the `DiffPool` neural network. Network iteratively coarse the graph using the `DiffPoolLayer` in `n_layers` coarsening steps and node number fall with `poolrate`.\n",
    "\n",
    "1. Firstly, we extract the adjacency matrix and convert it to dense tensor\n",
    "2. Extract `batch_num_nodes` into python list.\n",
    "3. Encode atom features\n",
    "4. Iterate over pooling layers\n",
    "5. Take a mean of calculated vectors\n",
    "6. Predict the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c191bd93-dc3f-4580-b9ea-ef9fdfbb08e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffPool(nn.Module):\n",
    "    def __init__(self, atom_dim, hidden_dim, initial_assign_size=8, out_dim=1, n_layers=3, poolrate=0.5):\n",
    "        super(DiffPool, self).__init__()\n",
    "        self.poolrate = poolrate\n",
    "        self.initial_assign_size = initial_assign_size\n",
    "        self.atom_encoder = AtomEncoder(emb_dim=atom_dim)\n",
    "        self.atom_dim = atom_dim\n",
    "        input_dim = atom_dim\n",
    "        self.pool_layers = [DiffPoolLayer(input_dim, initial_assign_size, hidden_dim)]\n",
    "        for _ in range(n_layers - 1):\n",
    "            initial_assign_size = int(initial_assign_size * poolrate)\n",
    "            self.pool_layers.append(DiffPoolLayer(hidden_dim, initial_assign_size, hidden_dim))\n",
    "        self.pool_layers = nn.ModuleList(self.pool_layers)\n",
    "        self.fc = nn.Linear(hidden_dim, out_dim)\n",
    "  \n",
    "    def forward(self, graph):\n",
    "        adj = graph.adjacency_matrix(transpose=True, ctx=device)\n",
    "        batch_num_nodes = graph.batch_num_nodes().tolist()\n",
    "        h = self.atom_encoder(graph.ndata[\"feat\"])\n",
    "        sz = self.initial_assign_size\n",
    "        adj = adj.to_dense()\n",
    "        for i, layer in enumerate(self.pool_layers):\n",
    "            adj, h = layer(adj, h, batch_num_nodes)\n",
    "            batch_num_nodes = [sz] * graph.batch_size\n",
    "            sz = int(sz * self.poolrate)\n",
    "        h = dgl.ops.segment_reduce(torch.tensor(batch_num_nodes).to(device), h, 'mean')\n",
    "        return self.fc(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ef1bae-5382-4d84-9496-e9553b597a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffpool_model = DiffPool(8, 8, 16, out_dim=dataset.num_tasks).to(device)\n",
    "run(diffpool_model, \"diffpool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fbfa0e-86cc-460b-8e04-649615604b11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b9e8ef-8a3a-4784-b63b-95cc8cc71595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8848e601",
   "metadata": {},
   "source": [
    "### Task 6. Augmentation (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8754bcce-b66b-4e6d-b2b1-acf500b60cf9",
   "metadata": {},
   "source": [
    "<img src='https://raw.githubusercontent.com/netspractice/advanced_gnn/made2021/assignment_contrastive_learning/contrastive_learning.png' width=500>\n",
    "\n",
    "Source: https://arxiv.org/abs/2103.00111"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1772157-4d89-488c-b526-3d7c372e3816",
   "metadata": {},
   "source": [
    "Contrastive learning aims to learn representations by maximizing feature consistency under differently augmented views, that exploit data- or task-specific augmentations. In a case of graph representation learning, there are some augmentation techniques that can be used to produce graph embeddings for downstream tasks, say classification.\n",
    "\n",
    "Write a class `GraphAugmentation` with a function `transform` that takes a graph and returns an augmented graph. Types of augmentation:\n",
    "* `drop_nodes` â€” randomly drops a share of nodes with a given ratio\n",
    "* `pert_edges` â€” randomly perturbs (rewires) a share of edges with a given ratio\n",
    "* `attr_mask` â€” randomly masks a share of node attributes with a given ratio and a name in `ndata` collection\n",
    "* `rw_subgraph` â€” builds a subgraph based on random walk\n",
    "* `identical` â€”Â the same graph, no augmentation\n",
    "\n",
    "Augmentations are applied to graphs with self-loops, so keep self-loops during edges perturbation. Parallel edges are allowed after perturbation. A random walk subgraph is constructed by (1) adding a random starting node, (2) adding all its neighbors, (3) adding all neighbors of a random node in the subgraph and repeating the step 3 while number of nodes exceeds the threshold `(1 - ratio)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b8242f-1a49-4c83-9dad-1c18f2ed7904",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6b9f812ad0bd212505b5b0e4f75e6c11",
     "grade": false,
     "grade_id": "cell-e7ac70ee8680c85f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GraphAugmentation():\n",
    "    def __init__(self, type, ratio=0.2, node_feat='attr'):\n",
    "        self.type = type\n",
    "        self.ratio = ratio\n",
    "        self.node_feat = node_feat\n",
    "    \n",
    "    def transform(self, g):\n",
    "        if self.type == 'drop_nodes':\n",
    "            return self.drop_nodes(g)\n",
    "        elif self.type == 'pert_edges':\n",
    "            return self.pert_edges(g)\n",
    "        elif self.type == 'attr_mask':\n",
    "            return self.attr_mask(g)\n",
    "        elif self.type == 'rw_subgraph':\n",
    "            return self.rw_subgraph(g)\n",
    "        elif self.type == 'identical':\n",
    "            return g\n",
    "    \n",
    "    def drop_nodes(self, g):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def pert_edges(self, g):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def attr_mask(self, g):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def rw_subgraph(self, g):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84da4bea-b450-41a9-bc18-edf164309adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = dgl.rand_graph(100, 300)\n",
    "g = g.remove_self_loop()\n",
    "g = g.add_self_loop()\n",
    "g.ndata['attr'] = torch.ones(100, 10)\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569954c5-c101-4df9-a911-c112d91e4053",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "abae1bfb701e283a75a6b5b41367e412",
     "grade": true,
     "grade_id": "cell-fdde4921c202a6ca",
     "locked": true,
     "points": 0.75,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ratio = 0.7\n",
    "aug = GraphAugmentation('drop_nodes', ratio=ratio)\n",
    "aug_g = aug.transform(g)\n",
    "assert aug_g.ndata['attr'].shape[1] == g.ndata['attr'].shape[1]\n",
    "assert aug_g.ndata['attr'].shape[0] < g.ndata['attr'].shape[0]\n",
    "assert aug_g.ndata['attr'].shape[0] == int(g.number_of_nodes() * (1 - ratio))\n",
    "G = nx.Graph(aug_g.to_networkx())\n",
    "assert np.isclose(nx.laplacian_spectrum(G), 0).sum() > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561e58f5-893f-4290-9d53-b23aa90e8c8b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "53fad1eceea54fe33fae37c5c90f3ed5",
     "grade": true,
     "grade_id": "cell-3c73be416d8accf1",
     "locked": true,
     "points": 0.75,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "aug = GraphAugmentation('pert_edges', ratio=0.2)\n",
    "aug_g = aug.transform(g)\n",
    "assert aug_g.ndata['attr'].shape == g.ndata['attr'].shape\n",
    "assert aug_g.number_of_edges() == g.number_of_edges()\n",
    "assert not torch.all(aug_g.adj().to_dense() == g.adj().to_dense())\n",
    "assert torch.all(aug_g.adj().to_dense().diag() >= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed59342e-2e05-4f37-8258-8378796b8dd2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e269174ce437ca68db5a407a8fc0de3e",
     "grade": true,
     "grade_id": "cell-c4161357d93cac21",
     "locked": true,
     "points": 0.75,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "aug = GraphAugmentation('attr_mask', ratio=0.2, node_feat='attr')\n",
    "aug_g = aug.transform(g)\n",
    "assert aug_g.ndata['attr'].shape == (100, 10)\n",
    "mask = (aug_g.ndata['attr'][0, :] == 0).repeat(100, 1)\n",
    "assert torch.all(aug_g.ndata['attr'][mask] == 0)\n",
    "assert torch.all(aug_g.ndata['attr'][~mask] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3496abd-9172-4ec2-97e6-815cea4b274f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "284790210535ea8142d87ad8868e535f",
     "grade": true,
     "grade_id": "cell-88caaf0b9bfc11dc",
     "locked": true,
     "points": 0.75,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "aug = GraphAugmentation('rw_subgraph', ratio=0.7)\n",
    "aug_g = aug.transform(g)\n",
    "assert aug_g.ndata['attr'].shape[1] == g.ndata['attr'].shape[1]\n",
    "assert aug_g.ndata['attr'].shape[0] < g.ndata['attr'].shape[0]\n",
    "G = nx.Graph(aug_g.to_networkx())\n",
    "assert np.isclose(nx.laplacian_spectrum(G), 0).sum() == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14614f8f-664e-48a4-a62b-f2f1fe74304e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b66a7107",
   "metadata": {},
   "source": [
    "### Task 7. Contrastive dataset (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9e4e9d-3c90-4c29-aec2-5a406e859975",
   "metadata": {},
   "source": [
    "We will fed augmented graphs into encoder during training to obtain graph embeddings. Let us prepare a graph contrastive dataset class so that each element in the dataset will represent augmented graphs and a label.\n",
    "\n",
    "Write a class `ContrastiveDataset` with a function `__getitem__` that takes a graph's index and returns a tuple:\n",
    "* an initial graph\n",
    "* a graph after the first augmentation\n",
    "* a graph after the second augmentation\n",
    "* a label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439445dc-69ee-4223-80b9-41286c3efe68",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5bb13505b068d1110d51d5027d6c3ce5",
     "grade": false,
     "grade_id": "cell-42dd50524514a69a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ContrastiveDataset(DGLDataset):\n",
    "    def __init__(self, filename, augmentations):\n",
    "        self.filename = filename\n",
    "        self.graphs = None\n",
    "        self.labels = None\n",
    "        self.augmentations = augmentations\n",
    "        assert len(self.augmentations) == 2\n",
    "        super().__init__(name=filename)\n",
    "\n",
    "    def process(self):\n",
    "        graphs, graph_data = dgl.load_graphs(self.filename)\n",
    "        self.graphs = graphs\n",
    "        self.labels = graph_data['labels']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c1e1b1-825b-4513-8d0b-c72ea98d5eea",
   "metadata": {},
   "source": [
    "PROTEINS is a dataset with 1113 proteins where nodes are secondary structure elements and there is an edge between two nodes if they are neighbors in the amino-acid sequence or in 3D space. It has 3 discrete labels, representing helix, sheet or turn. Proteins are divided into two classes: enzymes and non-enzymes. Source: https://arxiv.org/abs/2007.08663.\n",
    "\n",
    "Let us create a dataset with dropping nodes and masking attributes augmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beff3b6-956c-4bc7-abae-a8e763343400",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://github.com/netspractice/advanced_gnn/raw/made2021/assignment_contrastive_learning/proteins.bin'\n",
    "open('proteins.bin', 'wb').write(requests.get(url).content);\n",
    "\n",
    "augmentations = []\n",
    "augmentations.append(GraphAugmentation('drop_nodes', ratio=0.1))\n",
    "augmentations.append(GraphAugmentation('attr_mask', ratio=0.4, node_feat='attr'))\n",
    "dataset = ContrastiveDataset(filename='proteins.bin', augmentations=augmentations)\n",
    "N = len(dataset)\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9427fd3f-a6ae-45c9-b7e2-05ce279fe7ed",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c78801e16b68eb7693b87b87629c9a6d",
     "grade": true,
     "grade_id": "cell-d1f1379a953a2112",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "g, aug_g1, aug_g2, label = dataset[0]\n",
    "assert g.ndata['attr'].shape == (42, 3)\n",
    "assert aug_g1.ndata['attr'].shape == (37, 3)\n",
    "assert g.ndata['attr'].sum() <= 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f791da-a975-4a50-881e-f313ac41aa5d",
   "metadata": {},
   "source": [
    "Since we perform the random walk subgraph augmentation, we want to make sure all initial graphs are connected.\n",
    "\n",
    "Write a function `connected_subset` that takes an initial dataset and returns a `torch.utils.data.dataset.Subset` with connected graphs only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8499f030-7126-47c5-94af-f2260d246943",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5dc3193e43af065dbc7f4c549cc658c2",
     "grade": false,
     "grade_id": "cell-cd9aace17d526769",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def connected_subset(dataset):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d31daa-66a6-4a6d-9b3f-ba9d277d7d96",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "965acfaccf52fb36a89a9f1be02cb98e",
     "grade": true,
     "grade_id": "cell-2ab0df43d37ab94c",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "c_dataset = connected_subset(dataset)\n",
    "N = len(c_dataset)\n",
    "assert N == 1067"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49842d53-76ed-4936-b5b9-ae1640b6c930",
   "metadata": {},
   "source": [
    "Let us look at some graphs in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d604147a-7479-47dd-89ae-ec38e4574910",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['tab:orange', 'tab:green']\n",
    "plt.figure(figsize=(12, 12))\n",
    "np.random.seed(0)\n",
    "for i in range(16):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    g, _, _, l = c_dataset[np.random.randint(N)]\n",
    "    g = nx.Graph(g.to_networkx())\n",
    "    g.remove_edges_from(nx.selfloop_edges(g))\n",
    "    nx.draw_kamada_kawai(g, node_size=30, node_color=colors[l])\n",
    "    plt.title('enzymes' if l == 1 else 'non-enzymes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd5a5dd-0ac6-44b2-b3c0-9ad27472ba4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e88b7e65",
   "metadata": {},
   "source": [
    "### Task 8. Contrastive loss (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dbb8c1-cc4d-4717-bca6-aade98d0a3ed",
   "metadata": {},
   "source": [
    "Let an encoder be the two-layers GCN (`GraphConv` in `dgl`) with mean graph pooling and two-layers MLP projection head. All layers except of input and output ones have `hidden_dim` dimensionality. Apply ReLU as an activation function.\n",
    "\n",
    "Write a class `GCNEncoder` with a function `forward` that takes a batch of graphs, node attrubute name in `ndata` collection and returns graph embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ec811e-a97e-4825-ab6e-4d4681be86e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GCNEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = GraphConv(input_dim, hidden_dim, activation=nn.ReLU())\n",
    "        self.conv2 = GraphConv(hidden_dim, hidden_dim, activation=nn.ReLU())\n",
    "        self.pooling = AvgPooling()\n",
    "        self.proj_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, g, node_feat):\n",
    "        h = self.conv1(g, g.ndata[node_feat])\n",
    "        h = self.conv2(g, h)\n",
    "        h = self.pooling(g, h)\n",
    "        h = self.proj_head(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325af060-8117-487c-9d0e-297d2226bb4a",
   "metadata": {},
   "source": [
    "Let us check the logistic regression model on the untrained encoder output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56643ea5-9beb-4302-9c97-94756bf9e17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(c_dataset, train_idx, test_idx):\n",
    "    graph_train = []\n",
    "    y_train = []\n",
    "    for idx in train_idx:\n",
    "        graph, _, _, label = c_dataset[idx]\n",
    "        graph_train.append(graph)\n",
    "        y_train.append(label)\n",
    "    \n",
    "    graph_test = []\n",
    "    y_test = []\n",
    "    for idx in test_idx:\n",
    "        graph, _, _, label = c_dataset[idx]\n",
    "        graph_test.append(graph)\n",
    "        y_test.append(label)\n",
    "\n",
    "    graph_train = dgl.batch(graph_train).to(device)\n",
    "    graph_test = dgl.batch(graph_test).to(device)\n",
    "    y_train = torch.tensor(y_train).to(device)\n",
    "    y_test = torch.tensor(y_test).to(device)\n",
    "    \n",
    "    return graph_train, graph_test, y_train, y_test\n",
    "\n",
    "np.random.seed(0)\n",
    "ratio = [0.9, 0.1] # train test ratio\n",
    "split_idx = ['train'] * int(ratio[0] * N) \\\n",
    "    + ['test'] * int(ratio[1] * N)\n",
    "split_idx = np.random.permutation(split_idx)\n",
    "train_idx = np.where(split_idx == 'train')[0]\n",
    "test_idx = np.where(split_idx == 'test')[0]\n",
    "\n",
    "graph_train, graph_test, y_train, y_test = train_test_split(\n",
    "    c_dataset, train_idx, test_idx)\n",
    "assert graph_train.ndata['attr'].shape == (36363, 3)\n",
    "assert graph_test.ndata['attr'].shape == (3740, 3)\n",
    "assert y_train.shape == (960, )\n",
    "assert y_test.shape == (106, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea845b3-d9c5-41a6-aa1e-4e30360785ef",
   "metadata": {},
   "source": [
    "Let us check the classification score and look at tSNE visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7676ba5-90fe-4e00-a1a6-4dd502d2d052",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = GCNEncoder(input_dim=3, hidden_dim=32, output_dim=16)\n",
    "encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c598af19-5f2f-47f8-ba81-1aca66005a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_score(graph_train, graph_test, y_train, y_test, encoder, show=True):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        X_train = encoder(graph_train, 'attr').cpu()\n",
    "        X_test = encoder(graph_test, 'attr').cpu()\n",
    "    \n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train, y_train.cpu())\n",
    "    score = clf.score(X_test, y_test.cpu())\n",
    "    \n",
    "    if show:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        cmap = plt.cm.Set1_r\n",
    "        dec = TSNE(n_components=2)\n",
    "        xy_emb = dec.fit_transform(X_train)\n",
    "        plt.scatter(xy_emb[:, 0], xy_emb[:, 1], c=y_train.cpu(), cmap=cmap, s=5)\n",
    "        plt.title('tSNE visualization')\n",
    "        plt.show()\n",
    "        print('Accuracy: {:.4f}'.format(score))\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc5130e-23ae-44be-821b-7ae76300ac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = classification_score(graph_train, graph_test, y_train, y_test, encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f69cbb-37f8-457a-8585-8e240f5a4811",
   "metadata": {},
   "source": [
    "In graph contrastive learning, pre-training is performed through maximizing the agreement between two augmented views of the same graph via a contrastive loss in the latent space. Contrastive loss function is defined to enforce maximizing the consistency between positive pairs $z_i$, $z_j$ (the same graph under different augmentations) compared with negative pairs. Here we utilize the NT-Xent Loss that is defined for $n$-th graph in a batch of $N$ graphs as follows:\n",
    "\n",
    "$$l_{n}=-\\log \\frac{\\exp \\left(\\text{sim}(z_{n, i}, z_{n, j}) / \\tau \\right)}{\\sum_{n'=1, n' \\neq n}^{N} \\exp \\left( \\text{sim}(z_{n, i}, z_{n', j}) / \\tau \\right)}$$\n",
    "\n",
    "where $\\text{sim}$ is cosine similarity $\\text{sim}(z_i, z_j) = z_i^\\top z_j / (\\| z_i \\| \\cdot \\| z_j \\|)$ and $\\tau$ is a temperature parameter.\n",
    "\n",
    "Source: https://arxiv.org/pdf/2010.13902.pdf\n",
    "\n",
    "Write a function `ntxent` that takes a batch of agmented graph embeddings `x1` and a batch of agmented graph embeddings `x2` and returns mean loss value among all graphs $L = \\frac{1}{N}\\sum_{n=1}^N l_n$.\n",
    "\n",
    "_Hint: it is possible to use matrix operations only, with no loops._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2def76-1ea2-4467-be6c-4b99e6ca7942",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "81521cead5f908b9c9dc597bfd07b0a6",
     "grade": false,
     "grade_id": "cell-1db7f31037146f19",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ntxent(x1, x2, tau=0.1):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f50e789-9a4a-4e3c-8367-99e6ce3e9e0d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3081a7217dec262176b79c517891b05d",
     "grade": true,
     "grade_id": "cell-a1a7c76aa37ed72f",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x1 = torch.tensor([[1., 0.], [0., 1.]])\n",
    "x2 = torch.tensor([[1., 0.], [0., 1.]])\n",
    "assert ntxent(x1, x2) == -10\n",
    "\n",
    "x1 = torch.tensor([[1., 0.], [0., 1.]])\n",
    "x2 = torch.tensor([[0., 1.], [1., 0.]])\n",
    "assert ntxent(x1, x2) == 10\n",
    "\n",
    "torch.manual_seed(0)\n",
    "x1 = torch.randn(128, 16)\n",
    "x2 = torch.randn(128, 16)\n",
    "assert round(ntxent(x1, x2).item(), 4) == 7.191"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4980f9c-eea9-4029-b81c-a6d4c5f2cb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GraphDataLoader(\n",
    "    c_dataset,\n",
    "    batch_size=64,\n",
    "    drop_last=False,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c9f75f-52b6-477c-820f-6f7d979fde55",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = GCNEncoder(input_dim=3, hidden_dim=32, output_dim=16)\n",
    "encoder.to(device)\n",
    "opt = Adam(encoder.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164f943b-fbf7-4ad7-8aac-f3d913788a4a",
   "metadata": {},
   "source": [
    "Write a function `train` that takes augmented batches, makes optimization step and returns a loss value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b942b3-9f07-4a79-afaf-91568b6c46aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, aug_batch1, aug_batch2, opt):\n",
    "    z1 = encoder(aug_batch1.to(device), 'attr')\n",
    "    z2 = encoder(aug_batch2.to(device), 'attr')\n",
    "    loss = ntxent(z1, z2)\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5382d215-11fa-447d-8486-33c0f1b4037c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch, aug_batch1, aug_batch2, label in loader:\n",
    "    break\n",
    "loss_item = train(encoder, aug_batch1, aug_batch2, opt)\n",
    "assert type(loss_item) == float\n",
    "assert loss_item > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b4925b-76f7-4602-ba8e-69ad4ffd9ea1",
   "metadata": {},
   "source": [
    "Here is a training loop that accumulates mean loss per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd405b4-bcae-4eb0-b236-5a6676c32c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_vals = []\n",
    "n_epochs = 30\n",
    "for i in range(n_epochs):\n",
    "    loss_epoch = []\n",
    "    for batch, aug_batch1, aug_batch2, label in loader:\n",
    "        loss_item = train(encoder, aug_batch1, aug_batch2, opt)\n",
    "        loss_epoch.append(loss_item)\n",
    "    loss_vals.append(sum(loss_epoch)/len(loss_epoch))\n",
    "    plt.plot(loss_vals)\n",
    "    plt.title('Contrastive loss. Epoch: {}/{}'.format(i+1, n_epochs))\n",
    "    plt.show();\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb5aba3-447b-4768-95ca-66e9b27ecfbc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ec19a29a7066a1f9b7fca96f7869e8f6",
     "grade": true,
     "grade_id": "cell-d2b7f023c5c7a10c",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "score = classification_score(graph_train, graph_test, y_train, y_test, encoder)\n",
    "assert score > 0.65"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd276d67-ac27-4f4f-b065-618fbeb9920a",
   "metadata": {},
   "source": [
    "As we see, we can noticeably improve classification score using self-supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22efb5a4",
   "metadata": {},
   "source": [
    "### Task 9. Tail prediction (1 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accfca76-8498-4e7e-bc1c-a74451545479",
   "metadata": {},
   "source": [
    "Here we aim to compare augmentation techniques and conclude which pair is better for PROTEINS dataset.\n",
    "\n",
    "Write a function `run` that takes a filename with proteins, number of epochs and a list of types of augmentation. It returns a np.array with a classification score matrix where rows are first augmentation, columns are second augmentation. Since the matrix is asymptotically symmetric, calculate the upper triangle values only.\n",
    "\n",
    "It can take time. To speed up the evaluation, return calculated score matrix without actual training:\n",
    "```\n",
    "def run(filename, n_epochs, augs):\n",
    "    scores = [[0.5, 0.5, 0.5, 0.5, 0.5], [0, 0.5, 0.5, 0.5], ...\n",
    "    return scores\n",
    "\n",
    "    ### ACTUAL TRAINING\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddde81cb-7bc8-4336-90fe-7bbff203dee8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c884c5c8ede0a63a76085a61822bc7b3",
     "grade": false,
     "grade_id": "cell-3ed7b9c2930e0f04",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run(filename, n_epochs, augs):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c41aab-da27-4307-9186-7de3aef2dede",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb072cdb741dfd95bfcf07747ed95326",
     "grade": true,
     "grade_id": "cell-bcf12753c9064768",
     "locked": true,
     "points": 1.0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "augs = ['drop_nodes', 'pert_edges', 'attr_mask', 'rw_subgraph', 'identical']\n",
    "res = run('proteins.bin', n_epochs=30, augs=augs)\n",
    "symm = (res.T + res - np.diag(res[range(5), range(5)])).sum(0)\n",
    "assert np.all((res > 0).sum(0) == np.arange(5) + 1)\n",
    "assert np.all(res[res > 0] > 0.6)\n",
    "assert symm[0] > symm[4]\n",
    "assert symm[2] > symm[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3737ac-a194-4fce-a9ef-03a07c039397",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(res, index=augs, columns=augs).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfa7c1b-b2d8-4c49-9b02-16f1dcafec1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd817886",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}